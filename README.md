# AI-Journey
Tracking progress in AI research and my journey as a student.

## Interesting papers I've read

**Pre-Attention**
* [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) (2014)
* [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784) (2014)

**2017**
* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

**2018**
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

**2019**
* [Language Models are Unsupervised Multitask Learners](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask) - GPT2

**2020**
* [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) - GPT3
* [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929g) - ViT

**2021**
* [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601v4)

**2023**
* [Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation](https://paperswithcode.com/paper/self-taught-optimizer-stop-recursively-self)
* [Large Language Models Are Zero-Shot Time Series Forecasters](https://arxiv.org/abs/2310.07820)
