# AI-Journey
Tracking progress in AI research and my journey as a student. Thank you to my teachers and mentors.

## Interesting papers I've read

**My interest in ML began here**
* [A high-bias, low-variance introduction to Machine Learning for physicists](https://arxiv.org/abs/1803.08823)

**Pre-Attention**
* [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546) - Word2Vec (2013)
* [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) (2014)
* [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784) (2014)
* [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) - AlexNet

**2017**
* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) - Transformers

**2018**
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) - BERT

**2019**
* [Language Models are Unsupervised Multitask Learners](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask) - GPT2

**2020**
* [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) - GPT3
* [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929g) - ViT
* [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361)

**2021**
* [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601v4)
* [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) - LoRa 

**2022**
* [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903) - CoT

**2023**
* [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf) - CoT
* [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/pdf/2308.08155.pdf)
* [Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation](https://paperswithcode.com/paper/self-taught-optimizer-stop-recursively-self)
* [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/pdf/2304.03442.pdf) - Smallville
* [Large Language Models Are Zero-Shot Time Series Forecasters](https://arxiv.org/abs/2310.07820) - LLMTime
* [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/pdf/2312.11805.pdf) - Gemini
* [(VIDEO) Sebastien Bubeck -- Sparks of AGI: early experiments with GPT-4](https://www.youtube.com/watch?v=qbIk7-JPB2c)
* [(VIDEO) Andrej Karpathy -- Intro to Large Language Models](https://www.youtube.com/watch?v=zjkBMFhNj_g&t=2s) - LLM OS
* [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290) - DPO

**2024**
* [Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding](https://arxiv.org/abs/2401.12954)
* [ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution](https://arxiv.org/pdf/2402.01145.pdf)
* [Video generation models as world simulators](https://openai.com/research/video-generation-models-as-world-simulators) - OpenAI's Sora
* [(VIDEO) Cognition Labs -- Introducing Devin, the first AI software engineer](https://www.youtube.com/watch?v=fjHtjT7GO1c)
* Pika, Figure AI, DBRX, Anthropic's Claude 3 Opus
* [KAN: Kolmogorov–Arnold Networks](https://arxiv.org/pdf/2404.19756)
* [Deepmind: AI Solves IMO Problems at Silver Medal Level](https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/)
* Nobel Prize in Physics: John J. Hopfield and Geoffrey Hinton “for foundational discoveries and inventions that enable machine learning with artificial neural networks”
* [OpenAI: o3 solves ARC-AGI at 88%](https://arcprize.org/blog/oai-o3-pub-breakthrough) - Reasoning models

**2025**
* [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948)
* [Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach](https://arxiv.org/abs/2502.05171)
* [(VIDEO) Andrej Karpathy -- How I Use LLMs](https://www.youtube.com/watch?v=EWvNQjAaOHw)
* Claude Sonnet 3.7, Grok 3, DeepSeek R1, GPT-4o, as of March 2025

